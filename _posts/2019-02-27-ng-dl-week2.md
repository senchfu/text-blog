---
layout: article
title: 吴恩达《深度学习》第二周 神经网络基础 笔记
key: ngdlweek2
sharing: true
tags: deep learning, notes
---

## 2.3 logistic回归的损失函数
 **损失函数（Loss Function或者Error Function）和成本函数（Cost Function）在概念上有所不同**  
 假设训练集包含m个样本，则损失函数是单个样本的预测值和真实值之间的误差，而成本函数则是所有m个样本的预测值与真实值的平均误差。损失函数通常使用$L()$来表示，成本函数通常使用$J()$来表示。  
 **梯度下降更适用于寻找凸函数的最优值**
 因为凸函数（convex function）与凹函数（concave function）相比有较少的局部最优值，意味着当损失函数是凸函数时使用梯度下降求最小值不容易陷入局部最优。  
## 2.8 计算图中的导数计算
**反向传播算法（BP）的本质就是微积分中的链式法则** 
假设成本函数$J(a,b,c)=3(a+b*c)$，则计算$J(a,b,c)$可以分3步：
1. 令变量$u=b*c$
2. 令变量$v=a+u$
3. $J=3*v$就是结果

该过程可以使用下面的计算图表示：

![计算图](https://blog-pictures-1254096412.cos.ap-shanghai.myqcloud.com/2019/02/27/%E9%80%89%E5%8C%BA_002.png)

如果我们已经知道了$J(a,b,c)$的值，想求$\frac{\partial J}{\partial a}$，则可以使用链式法则

$$\frac{\partial J}{\partial a}=\frac{\partial J}{\partial v}*\frac{\partial v}{\partial a}$$

这在计算图中是一个反向的过程（从右到左，在下图中使用红色箭头表示）

![反向传播](https://blog-pictures-1254096412.cos.ap-shanghai.myqcloud.com/2019/02/27/%E9%80%89%E5%8C%BA_003.png)

所以在神经网络中广泛使用的反向传播算法（BP）的本质就是链式法则。

## 2.11~2.14 向量化
**使用python实现各种模型时，要尽量不用或者少用显式for循环**
在实现模型时经常要对向量或者矩阵进行运算，在python中不推荐使用显式for循环来实现这些运算，可以使用python库numpy中的np.dot(u,v)等函数来代替显式for循环，这叫做**向量化（vvectorization）**。向量化可以使代码更加简洁，更重要的是，向量化的运行速度要比for循环快得多。  
## 2.15 Python中的广播
在python中可以使用广播（broadcast）来代替显式for循环从而简化矩阵的操作（加减乘除）。广播的常用情形如下：
1、u为矩阵（假设为mxn），v为常数，则u和v进行运算时，先将v扩充成mxn的矩阵，然后再与u进行矩阵运算。  
```python
import numpy as np

u = np.array([1,2,3])
v = 1
print(u+v)

u = np.array([[1,2,3],[4,5,6]])
v=1
print(u+v)
```
输出：  
```
[2 3 4]
[[2 3 4]
 [5 6 7]]
```
2、u为mxn的矩阵，v为1xn的向量（或者mx1），则u和v在运算时，先将v扩充成mxn的矩阵，然后再与v进行矩阵运算。  
```python
import numpy as np

u = np.array([[1,2,3],[4,5,6]])
v = np.array([1,2,3])
print(u+v)

u = np.array([[1,2,3],[4,5,6]])
v = np.array([[1],[2]])
print(u+v)
```
输出：  
```
[[2 4 6]
 [5 7 9]]
[[2 3 4]
 [6 7 8]]
```